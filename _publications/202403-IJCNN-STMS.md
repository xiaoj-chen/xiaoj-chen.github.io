---
title: "STMS: An Out-Of-Distribution Model Stealing Method Based on Causality (ACCEPTED)"
collection: publications
category: pubpapers
permalink: /publication/202403-IJCNN-STMS
date: 2024-03-18
venue: 'International Joint Conference on Neural Networks (IJCNN)'
#paperurl: ''
---
**Authors:** Yunfei Yang, **Xiaojun Chen** etc...

**KeyWords:** *Model Stealing and Defending*

**Abstract:** Machine learning, particularly deep learning, is extensively
applied in various real-life scenarios. However, recent research has high lighted the severe infringement of privacy and intellectual property caused by model stealing attacks. Therefore, more researchers are dedicated to studying the principles and methods of such attacks to promote the security development of artificial intelligence. However, most of the existing attack methods rely heavily on prior information of the attacked models and consider ideal conditions. In order to better understand and defend against model stealing in real-world scenarios, we propose a novel model stealing attack method, named STMS, based on causal inference learning. For the first time, we introduce the problem of out-of-distribution generalization into the model stealing domain. The proposed approach operates under more challenging conditions, where the training and testing data of the target model are unknown, its internal information (structure, parameters, and gradients) is inaccessible, only hard labels of the output results are available, and there is a distribution shift during the testing phase. STMS achieves comparable or even better stealing and generalization performance than current mainstream works on multiple datasets and tasks. Moreover, this universal framework can be applied to improve the effectiveness of other model stealing methods and can be migrated to other areas of machine learning.
